# --- START OF FILE updater.py ---

import json
import csv
import os
import requests
import sys
import re
from pypinyin import lazy_pinyin 

# 1. è§£å†³åŒçº§å¯¼å…¥é—®é¢˜
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
import hero_scraper as crawler
import tier_scraper     # å¼•å…¥æµ·å…‹æ–¯åˆ†çº§çˆ¬è™«æ¨¡å—

# 2. è§£å†³è·¯å¾„é—®é¢˜
BASE_DIR = os.path.dirname(current_dir)
DATA_DIR = os.path.join(BASE_DIR, 'data')

# é…ç½®è·¯å¾„
CHAMPION_ID_FILE = os.path.join(DATA_DIR, "champions.json")
PINYIN_FILE      = os.path.join(DATA_DIR, "pinyin_map.json")
CSV_FILE         = os.path.join(DATA_DIR, "hero_augments.csv")
TIERS_FILE       = os.path.join(DATA_DIR, "tiers.json") 
CSV_HEADER       =["ä¸­æ–‡å", "è‹±æ–‡å", "åºå·", "æµ·å…‹æ–¯åç§°"]

# ================= 1. æ•°æ®çœŸç†åŒæ­¥ =================
def sync_official_data():
    print(">>> [1/4] æ­£åœ¨åŒæ­¥å®˜æ–¹è‹±é›„æ•°æ®...")
    try:
        ver_url = "https://ddragon.leagueoflegends.com/api/versions.json"
        version = requests.get(ver_url).json()[0]
        print(f"    å½“å‰æ¸¸æˆç‰ˆæœ¬: {version}")

        champ_url = f"https://ddragon.leagueoflegends.com/cdn/{version}/data/zh_CN/champion.json"
        data = requests.get(champ_url).json()['data']

        official_en_to_cn = {}
        official_cn_to_en = {}
        for en_id, info in data.items():
            cn_name = info['name']
            official_en_to_cn[en_id] = cn_name
            official_cn_to_en[cn_name] = en_id

        old_en_to_cn = {}
        if os.path.exists(CHAMPION_ID_FILE):
            with open(CHAMPION_ID_FILE, 'r', encoding='utf-8') as f:
                old_cn_to_en = json.load(f)
                old_en_to_cn = {en: cn for cn, en in old_cn_to_en.items()}

        with open(CHAMPION_ID_FILE, 'w', encoding='utf-8') as f:
            json.dump(official_cn_to_en, f, indent=4, ensure_ascii=False)
        
        new_champs = []
        renamed_champs =[]
        
        for en_id, cn_name in official_en_to_cn.items():
            if en_id not in old_en_to_cn:
                new_champs.append(en_id)
            elif old_en_to_cn[en_id] != cn_name:
                renamed_champs.append(en_id)
        
        print(f"    åŒæ­¥å®Œæˆã€‚å…± {len(official_en_to_cn)} ä¸ªè‹±é›„ã€‚")
        if new_champs:
            print(f"    ğŸŒŸ å‘ç° {len(new_champs)} ä¸ªå…¨æ–°è‹±é›„: {', '.join([official_en_to_cn[en] for en in new_champs])}")
        if renamed_champs:
            print(f"    âœï¸ å‘ç° {len(renamed_champs)} ä¸ªæ”¹åè‹±é›„: {', '.join([official_en_to_cn[en] for en in renamed_champs])}")
            
        return official_en_to_cn, official_cn_to_en, new_champs, renamed_champs

    except Exception as e:
        print(f"!!! å®˜æ–¹æ•°æ®åŒæ­¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ: {e}")
        return {}, {}, [],[]

# ================= 2. æ‹¼éŸ³ç”Ÿæˆ =================
def update_pinyin_file(official_cn_to_en):
    print(">>>[2/4] æ›´æ–°æ‹¼éŸ³æ£€ç´¢æ–‡ä»¶...")
    pinyin_data = {}
    for cn_name in official_cn_to_en.keys():
        pinyin_list = lazy_pinyin(cn_name)
        initials = "".join([p[0].lower() for p in pinyin_list if p])
        pinyin_data[cn_name] = initials
    
    with open(PINYIN_FILE, 'w', encoding='utf-8') as f:
        json.dump(pinyin_data, f, indent=4, ensure_ascii=False)
    print("    æ‹¼éŸ³æ–‡ä»¶å·²æ›´æ–°ã€‚")

# ================= 3. æ•°æ®ä¿æŠ¤é€»è¾‘ (è¯»CSV) =================
def load_csv_history():
    print(">>> [3/4] è¯»å–æœ¬åœ°å†å²æ•°æ® (æ•°æ®ä¿æŠ¤)...")
    history = {}
    if not os.path.exists(CSV_FILE):
        return history

    try:
        with open(CSV_FILE, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                en_name = row.get('è‹±æ–‡å')
                if en_name:
                    if en_name not in history:
                        history[en_name] = []
                    history[en_name].append(row)
        print(f"    å·²åŠ è½½ {len(history)} ä¸ªè‹±é›„çš„å†å²æ•°æ®ã€‚")
    except Exception as e:
        print(f"âš ï¸ è¯»å–å†å²CSVæ—¶å‡ºé”™ (å¯èƒ½æ˜¯ç©ºæ–‡ä»¶): {e}")
    
    return history

# ================= 4. åˆå¹¶ä¸ä¿å­˜ =================
def merge_and_save(official_en_to_cn, history_data, new_crawl_data):
    print("\n>>> [4/4] æ‰§è¡Œæ•°æ®åˆå¹¶ä¸æŒä¹…åŒ–...")
    final_rows = []
    missing_data_champions =[]

    official_cn_to_en = {cn: en for en, cn in official_en_to_cn.items()}
    crawl_by_en = {official_cn_to_en.get(cn, cn): data for cn, data in new_crawl_data.items()}

    for en_name, cn_name in official_en_to_cn.items():
        rows_to_write =[]

        if en_name in crawl_by_en:
            for item in crawl_by_en[en_name]:
                rows_to_write.append({
                    "ä¸­æ–‡å": cn_name,
                    "è‹±æ–‡å": en_name,
                    "åºå·": item['index'],
                    "æµ·å…‹æ–¯åç§°": item['name']
                })
        elif en_name in history_data:
            rows_to_write = history_data[en_name]
            for row in rows_to_write:
                row['ä¸­æ–‡å'] = cn_name
        else:
            missing_data_champions.append(cn_name)
        
        if rows_to_write:
            final_rows.extend(rows_to_write)

    try:
        with open(CSV_FILE, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=CSV_HEADER)
            writer.writeheader()
            writer.writerows(final_rows)
        print(f"âœ… å†™å…¥å®Œæˆï¼ä¸»æ–‡ä»¶: {CSV_FILE} (å…± {len(final_rows)} æ¡æ•°æ®)")
    except Exception as e:
        print(f"âŒ å†™å…¥ä¸»æ–‡ä»¶å¤±è´¥: {e}")
        
    if missing_data_champions:
        print(f"\nâš ï¸ æ³¨æ„: æœ‰ {len(missing_data_champions)} ä¸ªè‹±é›„å®Œå…¨æ²¡æœ‰ä»»ä½•æ•°æ®: {', '.join(missing_data_champions)}")

# ================= ä¸»ç¨‹åº =================
def main():
    print("=== ARAM æ•°æ®è‡ªåŠ¨ç»´æŠ¤ç®¡ç†å™¨ v8.0 (èœå•åˆ†ç¦»ç‰ˆ) ===\n")

    # 1. è‡ªåŠ¨æ‰§è¡ŒåŸºç¡€è®¾æ–½åŒæ­¥ï¼ˆæ¯æ¬¡å¿…æ‰§è¡Œï¼Œé€Ÿåº¦å¾ˆå¿«ï¼‰
    official_en_to_cn, official_cn_to_en, new_champs, renamed_champs = sync_official_data()
    if not official_en_to_cn:
        return

    update_pinyin_file(official_cn_to_en)

    # 2. æ ¸å¿ƒèœå•é€‰æ‹©
    print("\nè¯·é€‰æ‹©è¦æ‰§è¡Œçš„ä»»åŠ¡:")
    print("   [1] è‹±é›„æ•°æ®ï¼šæ™ºèƒ½å¢é‡ (è‡ªåŠ¨çˆ¬å–: å…¨æ–°è‹±é›„ + æ”¹åè‹±é›„ + æœ¬åœ°æ— æ•°æ®çš„è‹±é›„)")
    print("   [2] è‹±é›„æ•°æ®ï¼šå…¨é‡æ›´æ–° (å¼ºåˆ¶é‡æ–°çˆ¬å–æ‰€æœ‰è‹±é›„ï¼Œè€—æ—¶è¾ƒé•¿)")
    print("   [3] è‹±é›„æ•°æ®ï¼šæé€Ÿè¡¥æ¼ (ä»…çˆ¬å–æœ¬åœ°æ— æ•°æ®çš„è‹±é›„)")
    print("   [4] è‹±é›„æ•°æ®ï¼šç²¾ç¡®æ‰“å‡» (æ‰‹åŠ¨è¾“å…¥æŒ‡å®šè‹±é›„åç§°è¿›è¡Œæ›´æ–°)")
    print("   [5] å…¨å±€æµ·å…‹æ–¯ï¼šä¸»åŠ¨æ‹‰å–æœ€æ–°æµ·å…‹æ–¯åˆ†çº§å­—å…¸ (Prismatic/Gold/Silver)")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (é»˜è®¤1): ").strip()
    if not choice:
        choice = '1'
        
    # --- åˆ†æ”¯ Aï¼šæ‰§è¡Œæµ·å…‹æ–¯åˆ†çº§çˆ¬å–ä»»åŠ¡ ---
    if choice == '5':
        print("\n>>> æ­£åœ¨æ‰§è¡Œ: æµ·å…‹æ–¯åˆ†çº§å­—å…¸æ›´æ–°ä»»åŠ¡...")
        tier_scraper.scrape_all_augments(TIERS_FILE)
        print("\nâœ… ä»»åŠ¡ç»“æŸã€‚")
        return # æ‰§è¡Œå®Œåˆ†æ”¯åŠŸèƒ½ï¼Œç›´æ¥é€€å‡ºè„šæœ¬

    # --- åˆ†æ”¯ Bï¼šæ‰§è¡Œè‹±é›„æµ·å…‹æ–¯çˆ¬å–ä»»åŠ¡ (1, 2, 3, 4) ---
    
    # å¿…é¡»è¦åŠ è½½å†å²æ•°æ®æ¥ä¿æŠ¤æ—§æ•°æ®
    history_data = load_csv_history()
    missing_champs = [en for en in official_en_to_cn if en not in history_data]
    target_list =[] 

    if choice == '2':
        target_list =[(cn, en) for en, cn in official_en_to_cn.items()]
    elif choice == '3':
        target_list = [(official_en_to_cn[en], en) for en in missing_champs]
    elif choice == '4':
        user_input = input("è¯·è¾“å…¥è¦æ›´æ–°çš„è‹±é›„åæˆ–è‹±æ–‡ID (å¤šä¸ªç”¨é€—å·æˆ–ç©ºæ ¼åˆ†éš”): ").strip()
        query_names = re.split(r'[,ï¼Œ\s]+', user_input)
        for q in query_names:
            if not q: continue
            matched_en = None
            for en, cn in official_en_to_cn.items():
                if q.lower() == en.lower() or q == cn:
                    matched_en = en
                    break
            if matched_en:
                target_list.append((official_en_to_cn[matched_en], matched_en))
            else:
                print(f"   [è­¦å‘Š] æ‰¾ä¸åˆ°å¯¹åº”çš„è‹±é›„: {q}")
        target_list = list(set(target_list))
    else: # é»˜è®¤æƒ…å†µ (é€‰é¡¹ 1)
        targets = set(new_champs + renamed_champs + missing_champs)
        target_list = [(official_en_to_cn[en], en) for en in targets]

    new_crawl_data = {}
    if target_list:
        print(f"\n>>> å‡†å¤‡çˆ¬å– {len(target_list)} ä¸ªç›®æ ‡è‹±é›„...")
        new_crawl_data, failed_list = crawler.crawl_champions(target_list)
        
        if failed_list:
            print(f"\nâš ï¸ æœ¬æ¬¡çˆ¬å–é­é‡å¤±è´¥çš„è‹±é›„: {failed_list}")
            print("    (æ— éœ€æ‹…å¿§ï¼Œç¨‹åºä¼šè‡ªåŠ¨å›é€€ä¿ç•™å®ƒä»¬åœ¨ CSV ä¸­çš„æ—§æ•°æ®ï¼)")
    else:
        print("\n>>> æ£€æŸ¥å®Œæ¯•ï¼Œæ²¡æœ‰éœ€è¦æ‰§è¡Œè‹±é›„çˆ¬å–ä»»åŠ¡çš„ç›®æ ‡ã€‚")

    # æ‰§è¡Œåˆå¹¶ä¸ä¿æŠ¤å¹¶å†™å…¥
    merge_and_save(official_en_to_cn, history_data, new_crawl_data)
    print("\nâœ… ä»»åŠ¡ç»“æŸã€‚")

if __name__ == "__main__":
    main()