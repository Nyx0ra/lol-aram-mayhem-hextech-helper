# --- START OF FILE updater.py ---

import json
import csv
import os
import requests
import sys
import re
from pypinyin import lazy_pinyin 

# 1. è§£å†³åŒçº§å¯¼å…¥é—®é¢˜
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
import hero_scraper as crawler

# 2. è§£å†³è·¯å¾„é—®é¢˜
BASE_DIR = os.path.dirname(current_dir)
DATA_DIR = os.path.join(BASE_DIR, 'data')

# é…ç½®è·¯å¾„
CHAMPION_ID_FILE = os.path.join(DATA_DIR, "champions.json")
PINYIN_FILE      = os.path.join(DATA_DIR, "pinyin_map.json")
CSV_FILE         = os.path.join(DATA_DIR, "hero_augments.csv")
CSV_HEADER       = ["ä¸­æ–‡å", "è‹±æ–‡å", "åºå·", "æµ·å…‹æ–¯åç§°"]

# ================= 1. æ•°æ®çœŸç†åŒæ­¥ =================
def sync_official_data():
    """
    ä»å®˜æ–¹è·å–æœ€æ–°æ•°æ®ã€‚
    è¿”å›: 
        official_en_to_cn: {è‹±æ–‡ID: ä¸­æ–‡å} (ç”¨äºå†…éƒ¨é€»è¾‘çš„ä¸»é”®å­—å…¸)
        official_cn_to_en: {ä¸­æ–‡å: è‹±æ–‡ID} (ç”¨äºä¿å­˜champions.json)
        new_champs:[è‹±æ–‡ID] (å…¨æ–°è‹±é›„)
        renamed_champs: [è‹±æ–‡ID] (æ”¹åè‹±é›„)
    """
    print(">>> [1/4] æ­£åœ¨åŒæ­¥å®˜æ–¹è‹±é›„æ•°æ®...")
    try:
        ver_url = "https://ddragon.leagueoflegends.com/api/versions.json"
        version = requests.get(ver_url).json()[0]
        print(f"    å½“å‰æ¸¸æˆç‰ˆæœ¬: {version}")

        champ_url = f"https://ddragon.leagueoflegends.com/cdn/{version}/data/zh_CN/champion.json"
        data = requests.get(champ_url).json()['data']

        official_en_to_cn = {}
        official_cn_to_en = {}
        for en_id, info in data.items():
            cn_name = info['name']
            official_en_to_cn[en_id] = cn_name
            official_cn_to_en[cn_name] = en_id

        # è¯»å–æœ¬åœ°æ—§æ•°æ®ï¼Œä»¥ è‹±æ–‡ID ä½œä¸ºä¸»é”®è¿›è¡Œå¯¹æ¯”
        old_en_to_cn = {}
        if os.path.exists(CHAMPION_ID_FILE):
            with open(CHAMPION_ID_FILE, 'r', encoding='utf-8') as f:
                old_cn_to_en = json.load(f)
                old_en_to_cn = {en: cn for cn, en in old_cn_to_en.items()}

        # è¦†ç›–ä¿å­˜ä¸ºæœ€æ–°çš„ champions.json
        with open(CHAMPION_ID_FILE, 'w', encoding='utf-8') as f:
            json.dump(official_cn_to_en, f, indent=4, ensure_ascii=False)
        
        # ç²¾å‡†è®¡ç®—å¢é‡ï¼šæ–°è‹±é›„ & æ”¹åè‹±é›„
        new_champs = []
        renamed_champs =[]
        
        for en_id, cn_name in official_en_to_cn.items():
            if en_id not in old_en_to_cn:
                new_champs.append(en_id)
            elif old_en_to_cn[en_id] != cn_name:
                renamed_champs.append(en_id)
        
        print(f"    åŒæ­¥å®Œæˆã€‚å…± {len(official_en_to_cn)} ä¸ªè‹±é›„ã€‚")
        if new_champs:
            print(f"    ğŸŒŸ å‘ç° {len(new_champs)} ä¸ªå…¨æ–°è‹±é›„: {', '.join([official_en_to_cn[en] for en in new_champs])}")
        if renamed_champs:
            print(f"    âœï¸ å‘ç° {len(renamed_champs)} ä¸ªæ”¹åè‹±é›„: {', '.join([official_en_to_cn[en] for en in renamed_champs])}")
            
        return official_en_to_cn, official_cn_to_en, new_champs, renamed_champs

    except Exception as e:
        print(f"!!! å®˜æ–¹æ•°æ®åŒæ­¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ: {e}")
        return {}, {}, [],[]

# ================= 2. æ‹¼éŸ³ç”Ÿæˆ =================
def update_pinyin_file(official_cn_to_en):
    print(">>>[2/4] æ›´æ–°æ‹¼éŸ³æ£€ç´¢æ–‡ä»¶...")
    pinyin_data = {}
    for cn_name in official_cn_to_en.keys():
        pinyin_list = lazy_pinyin(cn_name)
        initials = "".join([p[0].lower() for p in pinyin_list if p])
        pinyin_data[cn_name] = initials
    
    with open(PINYIN_FILE, 'w', encoding='utf-8') as f:
        json.dump(pinyin_data, f, indent=4, ensure_ascii=False)
    print("    æ‹¼éŸ³æ–‡ä»¶å·²æ›´æ–°ã€‚")

# ================= 3. æ•°æ®ä¿æŠ¤é€»è¾‘ (è¯»CSV) =================
def load_csv_history():
    """
    è¯»å–ç°æœ‰CSVåˆ°å†…å­˜ã€‚
    ã€é‡è¦æ”¹åŠ¨ã€‘ï¼šä½¿ç”¨ è‹±æ–‡å(en_name) ä½œä¸ºå­—å…¸çš„ Keyï¼Œé˜²æ­¢ä¸­æ–‡æ”¹åå¯¼è‡´æ‰¾ä¸åˆ°å†å²æ•°æ®ã€‚
    è¿”å›ç»“æ„: {en_name:[row_dict, ...]}
    """
    print(">>> [3/4] è¯»å–æœ¬åœ°å†å²æ•°æ® (ä¸»é”®: è‹±æ–‡ID)...")
    history = {}
    if not os.path.exists(CSV_FILE):
        return history

    try:
        with open(CSV_FILE, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                en_name = row.get('è‹±æ–‡å')
                if en_name:
                    if en_name not in history:
                        history[en_name] = []
                    history[en_name].append(row)
        print(f"    å·²åŠ è½½ {len(history)} ä¸ªè‹±é›„çš„å†å²æ•°æ®ã€‚")
    except Exception as e:
        print(f"âš ï¸ è¯»å–å†å²CSVæ—¶å‡ºé”™ (å¯èƒ½æ˜¯ç©ºæ–‡ä»¶): {e}")
    
    return history

# ================= 4. åˆå¹¶ä¸ä¿å­˜ =================
def merge_and_save(official_en_to_cn, history_data, new_crawl_data):
    """
    ä»¥å®˜æ–¹è‹±æ–‡ID(en_name)ä¸ºæ ¸å¿ƒå¾ªç¯ï¼Œåˆå¹¶æ–°è€æ•°æ®ã€‚
    """
    print(">>> [4/4] æ‰§è¡Œæ•°æ®åˆå¹¶ä¸æŒä¹…åŒ–...")
    final_rows =[]
    missing_data_champions =[]

    # crawler è¿”å›çš„å­—å…¸ key å¯èƒ½æ˜¯ cn_nameï¼Œä¸ºäº†ç¨³å®šï¼Œæˆ‘ä»¬é€šè¿‡å®˜æ–¹æ˜ å°„æŠŠå®ƒè½¬æˆä»¥ en_name ä¸º key
    official_cn_to_en = {cn: en for en, cn in official_en_to_cn.items()}
    crawl_by_en = {official_cn_to_en.get(cn, cn): data for cn, data in new_crawl_data.items()}

    for en_name, cn_name in official_en_to_cn.items():
        rows_to_write =[]

        # ç­–ç•¥Aï¼šæœ¬æ¬¡çˆ¬å–æˆåŠŸï¼Œä½¿ç”¨æ–°æ•°æ®
        if en_name in crawl_by_en:
            for item in crawl_by_en[en_name]:
                rows_to_write.append({
                    "ä¸­æ–‡å": cn_name, # å§‹ç»ˆä½¿ç”¨å®˜æ–¹æœ€æ–°çš„ä¸­æ–‡å
                    "è‹±æ–‡å": en_name,
                    "åºå·": item['index'],
                    "æµ·å…‹æ–¯åç§°": item['name']
                })
        
        # ç­–ç•¥Bï¼šæœ¬æ¬¡æœªçˆ¬å–æˆ–çˆ¬å–å¤±è´¥ï¼Œä¿ç•™æ—§æ•°æ®ï¼ˆå®Œç¾æ•°æ®ä¿æŠ¤ï¼‰
        elif en_name in history_data:
            rows_to_write = history_data[en_name]
            # é¡ºæ‰‹æŠŠæ—§æ•°æ®é‡Œçš„ä¸­æ–‡åæ›´æ–°ä¸ºæœ€æ–°ç‰ˆï¼Œè§£å†³æ”¹åé—ç•™é—®é¢˜
            for row in rows_to_write:
                row['ä¸­æ–‡å'] = cn_name
        
        # ç­–ç•¥Cï¼šå½»åº•æ²¡æœ‰æ•°æ®
        else:
            missing_data_champions.append(cn_name)
        
        if rows_to_write:
            final_rows.extend(rows_to_write)

    # å†™å…¥ CSV
    try:
        with open(CSV_FILE, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=CSV_HEADER)
            writer.writeheader()
            writer.writerows(final_rows)
        print(f"âœ… å†™å…¥å®Œæˆï¼ä¸»æ–‡ä»¶: {CSV_FILE} (å…± {len(final_rows)} æ¡æ•°æ®)")
    except Exception as e:
        print(f"âŒ å†™å…¥ä¸»æ–‡ä»¶å¤±è´¥: {e}")
        
    if missing_data_champions:
        print(f"\nâš ï¸ æ³¨æ„: æœ‰ {len(missing_data_champions)} ä¸ªè‹±é›„å®Œå…¨æ²¡æœ‰ä»»ä½•æ•°æ®: {', '.join(missing_data_champions)}")

# ================= ä¸»ç¨‹åº =================
def main():
    print("=== ARAM æ•°æ®è‡ªåŠ¨ç»´æŠ¤ç®¡ç†å™¨ v6.0 (ä¸»é”®æ¶æ„ç‰ˆ) ===\n")

    # 1. åŒæ­¥å®˜æ–¹æ•°æ®
    official_en_to_cn, official_cn_to_en, new_champs, renamed_champs = sync_official_data()
    if not official_en_to_cn:
        return

    # 2. æ›´æ–°æ‹¼éŸ³
    update_pinyin_file(official_cn_to_en)

    # 3. åŠ è½½å†å²æ•°æ®
    history_data = load_csv_history()
    
    # è®¡ç®—ç¼ºå¤±æ•°æ®çš„è‹±é›„
    missing_champs =[en for en in official_en_to_cn if en not in history_data]

    # 4. é€‰æ‹©çˆ¬å–æ¨¡å¼
    print("\nè¯·é€‰æ‹©çˆ¬å–ç­–ç•¥:")
    print("   [1] æ™ºèƒ½å¢é‡ (è‡ªåŠ¨çˆ¬å–: å…¨æ–°è‹±é›„ + æ”¹åè‹±é›„ + æœ¬åœ°æ— æ•°æ®çš„è‹±é›„)")
    print("   [2] å…¨é‡æ›´æ–° (å¼ºåˆ¶é‡æ–°çˆ¬å–æ‰€æœ‰è‹±é›„ï¼Œè€—æ—¶è¾ƒé•¿)")
    print("   [3] æé€Ÿè¡¥æ¼ (ä»…çˆ¬å–æœ¬åœ°æ— æ•°æ®çš„è‹±é›„)")
    print("   [4] ç²¾ç¡®æ‰“å‡» (æ‰‹åŠ¨è¾“å…¥æŒ‡å®šè‹±é›„åç§°è¿›è¡Œæ›´æ–°)")
    
    choice = input("è¯·è¾“å…¥é€‰é¡¹ (é»˜è®¤1): ").strip()
    
    target_list =[] 

    if choice == '2':
        # å…¨é‡æ¨¡å¼
        target_list = [(cn, en) for en, cn in official_en_to_cn.items()]
    elif choice == '3':
        # æé€Ÿè¡¥æ¼æ¨¡å¼
        target_list =[(official_en_to_cn[en], en) for en in missing_champs]
    elif choice == '4':
        # ç²¾ç¡®æ‰“å‡»æ¨¡å¼
        user_input = input("è¯·è¾“å…¥è¦æ›´æ–°çš„è‹±é›„åæˆ–è‹±æ–‡ID (å¤šä¸ªç”¨é€—å·æˆ–ç©ºæ ¼åˆ†éš”): ").strip()
        query_names = re.split(r'[,ï¼Œ\s]+', user_input)
        for q in query_names:
            if not q: continue
            matched_en = None
            # å¿½ç•¥å¤§å°å†™è¿›è¡ŒåŒ¹é…
            for en, cn in official_en_to_cn.items():
                if q.lower() == en.lower() or q == cn:
                    matched_en = en
                    break
            if matched_en:
                target_list.append((official_en_to_cn[matched_en], matched_en))
            else:
                print(f"   [è­¦å‘Š] æ‰¾ä¸åˆ°å¯¹åº”çš„è‹±é›„: {q}")
        # å»é‡
        target_list = list(set(target_list))
    else:
        # é»˜è®¤ï¼šæ™ºèƒ½å¢é‡æ¨¡å¼
        # åˆå¹¶é›†åˆå¹¶å»é‡
        targets = set(new_champs + renamed_champs + missing_champs)
        target_list =[(official_en_to_cn[en], en) for en in targets]

    new_crawl_data = {}
    if target_list:
        print(f"\n>>> å‡†å¤‡çˆ¬å– {len(target_list)} ä¸ªç›®æ ‡è‹±é›„...")
        # è°ƒç”¨çˆ¬è™«
        new_crawl_data, failed_list = crawler.crawl_champions(target_list)
        
        if failed_list:
            print(f"\nâš ï¸ æœ¬æ¬¡çˆ¬å–é­é‡å¤±è´¥çš„è‹±é›„: {failed_list}")
            print("    (æ— éœ€æ‹…å¿§ï¼Œç¨‹åºä¼šè‡ªåŠ¨å›é€€ä¿ç•™å®ƒä»¬åœ¨ CSV ä¸­çš„æ—§æ•°æ®ï¼)")
    else:
        print("\n>>> æ£€æŸ¥å®Œæ¯•ï¼Œæ²¡æœ‰éœ€è¦æ‰§è¡Œçˆ¬å–ä»»åŠ¡çš„ç›®æ ‡ã€‚")

    # 5. åˆå¹¶å¹¶ä¿å­˜
    merge_and_save(official_en_to_cn, history_data, new_crawl_data)

if __name__ == "__main__":
    main()